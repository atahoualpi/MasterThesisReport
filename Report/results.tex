\chapter{Analysis}\label{ch:analysis}

\section{Results}
This section makes an assessment of the final product and its results by pointing out what went well and what could be improved.  


\subsection{Evaluation}
%\begin{itemize}
%\item \colorbox{pink}{do we think we are successful?}
%\item \colorbox{pink}{did we manage to do what we wanted?}
%\item \colorbox{pink}{did we prove what we wanted to prove?}
%\item \colorbox{pink}{is it really a useful tool?}
%\item \colorbox{pink}{who is it targeting as a user?}
%\end{itemize} 

Overall, the work of this thesis proved successful. Procedural audio is used to replace prerecorded sound effects in a virtual environment. The sounds are generated using modal synthesis and based on physical models which are excited by different interactions driven by physics events in the game engine. The tool implemented enables the sound designer to control the synthesizer by choosing different materials for every object, the roughness of its surface and size.

\paragraph{Sounding objects}
The tool provides ready-made Unity\textsuperscript{\textregistered} prefabs and their corresponding audio components attached. The sounds produced by the proposed prefabs are object-specific, meaning that the modal data they have assigned matches their physical properties. The effect of this is a sound which is tightly coupled with its corresponding audio source. However, one can use the modal data of a certain object for a different shaped object, if it fits their needs. 

A drawback that entails from the aforementioned is that incorporating new objects in the game scene that can benefit from the tool is not a straightforward task. To ease the process a detailed guide that walks through the steps to add new objects and obtain their modal parameters is available in the appendix \ref{ap:guide}. 

\paragraph{Sound variation}
Since objects were separated into different sounding areas, it was easier to assume that all materials are homogeneous and isotropic. This assumption decreases the realistic aspect of sound variation along an object's surface which would ideally produce a different sound in every location. The higher resolution of the object's surface the more accurate will the sound on its surface be. This however results in higher calculations and higher amount of necessary recordings for every one of the areas. Therefore, a compromise is needed between the amount of ``sound areas'' and sound variation. Section \ref{sec:discretization} explained how the objects were divided into areas with a heuristic approach. The sounds generated by the objects within a virtual scene are compelling from the point of view of the authors and thus validated this approach.



\paragraph{Synthetic sounds versus recordings\\}

Regarding the quality of the synthesized sounds it can be said that in general, sounds that present high damping are less suitable for modal synthesis than sounds with low damping. 

Sounds produced by highly damped materials such as plastic or wood present spectrograms where information is spread in a wide frequency range. In this thesis the ten most prominent modes were chosen which leads to loss of information and thus a decrease in the richness of the sound. Nevertheless, it was possible to synthesize sounds coming from these types of objects. For example, the wooden cutting board, used in this study, is broad in frequency domain as seen in the first row of figure \ref{fig:specs_cb}. However, the synthesized sounds' spectrograms show that our method is able to keep the most important information despite some frequency components loss. 

On the other hand, sounds coming from low damped materials such as glass or metal present very clear frequency peaks. As seen in the spectrograms of the wine glass recordings in the appendix \ref{ap:spectrograms}, few peaks stand out among the rest. For this reason most of the information can be synthesized within ten modes. It can be seen on the spectrograms corresponding to the synthetic sounds of the wine glass that the prominent peaks are present. This is also validated also through unofficial auditory perception tests. 


\begin{figure}[H]
  \centering
    \includegraphics[width=\textwidth]{specs/cb_rec_sin.png}
      \caption{Spectrograms of a cutting board object's real-wolrd recordings and synthesized impacts, using sinusoidal additive synthesis.}
      \label{fig:specs_cb}
\end{figure}

Without leaving the realm of low damped materials, it is worth mentioning that in some cases metallic sounds cause undesired distortions. More specifically, for Q-factor values over $3100$, a distorted sound can be heard when the object is hit in different areas  in a short time-lapse. The origin of this problem seems to be the change of the modal frequencies (as certain areas have different modes than others) that are fed into the filters while the previous sound is still ``ringing''. 


Other inaccuracies in the synthetic sounds may derive from both the recording and parameters extraction phase. This thesis does not calculate the residual sound, namely the differentiation between the real-world sound and the synthesized sound \cite{ren2013example}. Therefore, information that concerns mostly the beginning of the sound (when the collision happens) is not calculated and is said to be perceptually relevant to identify the sounding object \cite{freed1990auditory}. However, the reason it was not included was precisely because we did not want the noise produced by the striker (a drumstick in this case) to be present in the synthesized sound. This is because in the game scene the characteristics of the striking object are unknown. Another aspect that could have affected the synthetic sounds is again the sound of the drumstick that may have interfered in the data extraction phase.

\paragraph{Controlling parameters\\}

The tool itself is easy-to-use with the already existing objects as one can control sound properties through high level parameters that are intuitive for the user. Those controllers which are explained in section \ref{sec:UI} are neither \textit{weak} nor too \textit{strong}. As stated in \cite{jaffe1995ten}, weak parameters affect the result so little, that there is a possibility the designer will adjust them to arbitrary values as he is ``wandering in the dark''. On the other hand, when even a minuscule tweaking of a very strong parameter induces a big change, the designer will probably find it difficult to choose the value he wants. In this thesis certain functions were used to offer coherent sliders that allow an efficient control.

Using the provided Unity\textsuperscript{\textregistered} prefabs as reference, the same modal data can be transferred automatically to reflect variations in object scaling or surface roughness and between different materials assigned to the same object. Spectrograms of the synthesized sound produced by a struck cup with different sizes (from smaller to bigger) can be seen in figure \ref{fig:cup_sizes}. Resonant frequencies are inversely proportional to the size, thus the pitch goes down when the object gets bigger.

Figure \ref{fig:bottle_rough} displays waveforms of a rolling bottle with different surface roughness values. As the roughness parameter is increased, the sound generated by the bumps along the surface becomes more audible. It is noticeable by looking at the waveforms that the rougher the surface the louder the synthetic sounds derived from the micro-collisions are. This evidence matches with the graph \ref{fig:roughnessgraph} from section \ref{sec:rolling_synth} that shows how the amplitude of the impulses corresponding to the bumps increases with the roughness of the object.

Figure \ref{fig:bottle_materials} shows spectrograms of a bottle falling over several platforms with different materials assigned to it, starting from plastic on the left and ending with metal on the right. Although the impact sounds are produced at the same moment of time, it can be seen that the ``tails'' of the individual impacts last longer towards the rightmost spectrogram which corresponds to metal. This proves the fact that the lower the damping (as for metallic sounds) the longer the sound lasts.

\begin{figure}[H]
  \centering
    \includegraphics[width=\textwidth]{specs/cup_sizes.png}
      \caption{Spectrograms of a cup object with size variation (smaller, reference size and bigger respectively).}
      \label{fig:cup_sizes}
\end{figure}

\begin{figure}[H]
  \centering
    \includegraphics[width=\textwidth]{specs/sph_roughness.png}
      \caption{Waveforms of a bottle object with surface roughness variation (from very smooth to very rough), rolling on a platform.}
      \label{fig:bottle_rough}
\end{figure}

\begin{figure}[H]
  \centering
    \includegraphics[width=\textwidth]{specs/bottle_materials.png}
      \caption{Spectrograms of a bottle object with material variation (plastic, wooden, ceramic, glass and metallic respectively).}
      \label{fig:bottle_materials}
\end{figure}

\paragraph{Integration of the tool\\}

The tool's \gls{UI} has been implemented similar to Unity\textsuperscript{\textregistered}'s \gls{UI} so that game developers feel familiar with the controls and can use it without having to learn how to use a third party audio solution. The goal is to assign a procedurally generated sound to an object in the game scene as easily  as one can apply a material texture. Therefore, not only sound designers who desire more realistic sound effects are targeted, but also game developers who are used to the Unity\textsuperscript{\textregistered} Editor and have no further sound design knowledge. 

As a side note, it is important for the designer to remember to press the \textit{``Apply''} button, on the prefab, every time a change is made, otherwise it will be undone. 

\subsection{Which Synthesis Method Is Better?}
After analyzing the results of the listening experiments on several people, we concluded that there is no good or bad method to synthesize modal sounds. Each object, with its characteristic physical attributes, gives different results. However, a division per material is possible to arise as seen in table \ref{tab:method_mat}.

In appendix \ref{ap:spectrograms}, the spectrograms for one object of each material examined in this work are shown. Although there are a lot of differences, both synthesis methods follow similar behavior as the recordings, with filter-based method being closer to the recordings.

\subsection{CPU demands}
Synthesizing sounds for applications real-time instead of using a mass of prerecorded clips is a good solution to the storage problem of nowadays portable and limited in memory devices. On the other hand, it is challenging and requires a lot of CPU performance when usually audio is restricted to a low limit and most of it is given to graphics, physics and artificial intelligence (AI) \cite{lloyd2011sound}. 

In our tool, however, when profiling a demonstration of a wine bottle rolling down a number of oblique platforms (seen in figure \ref{fig:test_sc2}), using the \textit{Profiler Window} of Unity\textsuperscript{\textregistered} Editor we can see that except for the initialization at the beginning where scripts consume some CPU power, the rest of the demonstration remains stable in performance and around $100 fps$ (figure \ref{fig:profile}). This performance test was held on a laptop with 4 Intel\textregistered\ Core\texttrademark\ i7-6700HQ CPUs running at 2.60GHz, each with 2 hardware threads.

\begin{figure}[H]
  \centering
    \includegraphics[width=\textwidth]{profiling1_r.PNG}
      \caption{The profiler view of Unity\textsuperscript{\textregistered} Editor when a wine bottle rolls down a number of platforms.}
      \label{fig:profile}
\end{figure} 

When testing the limits of the tool, we found out that in Filter-based method a total of 16 objects can be active at the same time before audio gets distorted. The corresponding number for the Sinusoidal method is 12, since more calculations take place. Those numbers were obtained on the laptop mentioned above, using the \textit{Unity\textsuperscript{\textregistered} Audio Profiler window} and represent the amount of \textbf{``Playing Audio Sources''} without the \textbf{``Total Audio CPU''} exceeding $100\%$ (see figure \ref{fig:audio_profile}).

Overall, most of the calculations happen in the start of the application, leaving only the identification of object and the corresponding data sending to audio chain to happen in real-time. We can, therefore, confirm that procedural audio does not consume more calculation power than is provided for audio in game and application productions.   

\begin{figure}[H]
  \centering
    \includegraphics[width=\textwidth]{audio_profile_r.PNG}
      \caption{The audio profiler view of Unity\textsuperscript{\textregistered} Editor when a total of 16 objects are enabled in the scene using the filter-based method for audio synthesis.}
      \label{fig:audio_profile}
\end{figure}

\subsection{What did we do new?}
First of all, we conducted an examination between two different sound synthesis methods. The sinusoidal one follows the modal synthesis equation (see equation \ref{eq:modal_response}, while the filter-based does the calculations inside the band-pass filters. We also tested the quality of the methods on people, by performing perception experiments. In addition, we combined all three major contact sounds (impact, rolling, scratching) in a sound designing tool, available and ready-to-use for developers. Our goal was to bridge university research with industry by implementing a tool inside the game engine.

\section{Discussion}
In this section, further discussion about the tool's usability is made and also several future extensions.

\subsection{What did we do new?}
First of all, we conducted an examination between two different sound synthesis methods. The sinusoidal one follows the modal synthesis equation (see equation \ref{eq:modal_response}, while the filter-based does the calculations inside the band-pass filters. We also tested the quality of the methods on people, by performing perception experiments. In addition, we combined all three major contact sounds (impact, rolling, scratching) in a sound designing tool, available and ready-to-use for developers. Our goal was to bridge university research with industry by implementing a tool inside the game engine.

\subsection{Types of games the tool can be used to}
This tool can be used for development of all sorts of games that include object interaction. They can vary from indoor AR applications to open world environment games running in consoles. However, in its current form it is recommended to be used for indoor games, since all prefabs available are every day object found in a kitchen. Designers are, also, highly endorsed to use it for \gls{AR} applications, where the synthesized sounds' realism will be compared with the real sounds of the environment.

\subsection{Why our work can be used in AR/VR?}
Virtual and augmented reality are becoming more and more widespread technologies. There are multiple applications where they can prove to be very useful both in everyday life and entertainment. Focusing on \gls{AR}, this thesis' goal is to develop a framework for sounds produced by object interactions. The sounds are designed to be realistic and event-based, so they can adapt into an \gls{AR} environment where a big portion of the objects are indeed real. Furthermore, with everything being generated in real time, it is possible to use our tool on portable devices with small storage space.

\gls{VR} devices, although they do not have storage problems since they are connected to PCs, they, as well as \gls{AR}, require flexible sounds that can adapt to any scenario. In addition, both technologies have graphics in 3D space, so 3D sound is essential to conclude the experience.

\subsection{Future work}
\colorbox{pink}{put those two or not?}
\begin{itemize}
\item \colorbox{pink}{randomize initial phase so peaks of the sine wave don't line up and distort the sound (saturation)}
\item \colorbox{pink}{use only one recording and interpolate for all areas}
\end{itemize}

The implemented tool is able to transfer an object's physical attributes from recordings to synthesized sounds. However, it does not take into account several parameters that would make the synthesis more accurate and less expensive when it is not necessary to be very detailed (e.g. sounds that are produced far away from the listener). As described in \cite{corbett2007timbrefields}, not only the location of the excitation point controls the timbre of the sound, but also the location of the listener. The radiation, namely the energy transferred to the listener's ear, depends on his location and it is calculated through the \textit{acoustic transfer function}. 

Another parameter is the early spatialization of the sound. More specifically, instead of spatialize every sound after it is synthesized, another option would be to do it at the same time, using the location of the object and listener. This approach would save calculation power for sounds that are very far away from the listener. In addition, for the filter-based method this can be accomplished by cascading the band-pass filters with spatial filters. Moreover, level of detail should be handled for sound quality. For example, a produced sound, with a sound pressure under a threshold of 10dB for example, should not be synthesized very qualitatively.   

Furthermore, some future work for more realistic effects could be done. Room acoustics, for example calculating the reverberation of the sounds, could be examined, or make the objects destructible by assigning a more high-pitched sound for each of the fragments of the object.

\subsection{Discussion on Procedural Audio}

Using procedural audio in games is much more convenient and less expensive in storage space, than audio files. It is adaptive to several different scenarios without further change or additional memory needs, as it is flexible and dynamic. For example, with just a slider, size adjustment is achieved. In addition, it provides the ability of physics-based audio by adding only a few kilobytes of data space. Adding new sounds is less time consuming and it is better for mobile platforms that are limited in both storage space and computational power.

On the other hand, using audio files in a game gives better quality results, while procedural audio's sounds lack some information that makes them not very realistic. With procedural audio, there are also requirements for more processing power, to synthesize the sounds, and additional coding, which is, usually, not a familiar approach for sound designers.
\colorbox{pink}{not every sound is easy to be produce procedurally}
