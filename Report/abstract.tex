\chapter*{Abstract}

Audio that is consistent and in synchrony with the visual graphics in a game is of crucial importance. Prerecorded sounds lack the flexibility needed in a virtual environment. Procedural audio, on the other hand, generates highly dynamic audio that can handle unpredictable events, while solving the storage problem of portable devices. 

Typical sound interactions within a game consist of impact, rolling and scratching sounds. Two different ways of synthesizing these sounds are examined in this thesis, both deriving from modal synthesis. After extracting the modal data from real world recordings, they are fed either into a number of damped oscillators or a number of band-pass filters. To achieve sound variation along the object's surface, several recordings that correspond to a different ``sound area'' on the object are used.

A UI consistent with Unity\textsuperscript{\textregistered}'s system is implemented to achieve consistency and offer control over the synthesized sounds. Sound designers are provided with high level intuitive parameters which form a tool that generates physics-based procedural audio.

Based on perceptual tests, it has been noted that a single parameter enables to identify changes in material category and that sound variation along an object's surface is desirable and preferred over one single sound.
